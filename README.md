# COMP3004


This project investigates and compares the performance of Q-learning and Deep Q-learning agents in the CartPole environment using various learning rates. The study aims to analyze the strengths and limitations of each algorithm and identify the impact of hyperparameters on the agent's performance. The Q-learning agent uses a table to store Q-values, while the Deep Q-learning agent employs a neural network to approximate Q-values. The results indicate that a learning rate of 0.1 yields the best performance for the Q-learning agent, while a learning rate of 0.001 provides better and more consistent outcomes for the Deep Q-learning agent. The Deep Q-learning agent outperforms the Q-learning agent in terms of higher rewards and a more stable training process, but it is computationally expensive and more prone to overfitting. The study also highlights three significant hurdles encountered during the project: hyperparameter tuning, ensuring stability and convergence of the learning algorithms, and evaluating the agent's performance. Despite the limitations, the findings provide valuable insights into the performance of Q-learning and Deep Q-learning agents and contribute to a better understanding of reinforcement learning techniques' strengths and limitations.
